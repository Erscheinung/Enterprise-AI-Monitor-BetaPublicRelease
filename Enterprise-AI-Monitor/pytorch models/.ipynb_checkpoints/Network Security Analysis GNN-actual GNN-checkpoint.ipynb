{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08ff4cc-305b-4264-aeff-3dcec6291111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch version: 2.5.1\n",
      "CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715f22ff-3d78-4aba-ae85-a5ddd54741df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b54cf8-4e0f-4506-9e97-38a452e7a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (175341, 42) y shape: (175341,)\n",
      "Test  X shape: (82332, 42) y shape: (82332,)\n"
     ]
    }
   ],
   "source": [
    "# Adjust filenames as needed\n",
    "TRAIN_PATH = \"UNSW_NB15_training-set.csv\"\n",
    "TEST_PATH  = \"UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "# 1) Read CSVs\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# 2) Fill NaN in attack_cat\n",
    "train_df = train_df.assign(attack_cat=train_df['attack_cat'].fillna('Normal'))\n",
    "test_df = test_df.assign(attack_cat=test_df['attack_cat'].fillna('Normal'))\n",
    "\n",
    "# 3) Encode 'attack_cat' as the label\n",
    "attack_cat_encoder = LabelEncoder()\n",
    "train_df['attack_cat'] = attack_cat_encoder.fit_transform(train_df['attack_cat'])\n",
    "test_df['attack_cat'] = attack_cat_encoder.transform(test_df['attack_cat'])\n",
    "\n",
    "# 4) Handle categorical columns more robustly\n",
    "cat_cols = ['proto', 'service', 'state']\n",
    "for col in cat_cols:\n",
    "    # Get unique values from both train and test\n",
    "    combined_categories = pd.concat([train_df[col], test_df[col]]).unique()\n",
    "    le = LabelEncoder()\n",
    "    le.fit(combined_categories.astype(str))\n",
    "    \n",
    "    # Transform separately\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "\n",
    "# Drop unused cols\n",
    "drop_cols = ['id', 'label']\n",
    "for c in drop_cols:\n",
    "    if c in train_df.columns:\n",
    "        train_df.drop(columns=c, inplace=True)\n",
    "    if c in test_df.columns:\n",
    "        test_df.drop(columns=c, inplace=True)\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c != 'attack_cat']\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['attack_cat'].values\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['attack_cat'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train X shape:\", X_train.shape, \"y shape:\", y_train.shape)\n",
    "print(\"Test  X shape:\", X_test.shape,  \"y shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cb7c4a-c357-4664-be8a-2f9680d64514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instead of KNN, let's try to connect flows (rows) if they share the same (proto, service, state).\n",
    "# NOT SURE IF IT WILL WORK!\n",
    "\n",
    "# Let's do this for TRAIN. Each row is a node.\n",
    "# # The columns for proto, service, state in the *scaled* matrix are no longer \n",
    "# # integers, so let's get them from the unscaled dataframe. That means we do:\n",
    "# train_proto  = train_df['proto'].values\n",
    "# train_service= train_df['service'].values\n",
    "# train_state  = train_df['state'].values\n",
    "\n",
    "# # Group rows by (proto, service, state)\n",
    "# from collections import defaultdict\n",
    "# groups = defaultdict(list)\n",
    "# for i, (p, s, st) in enumerate(zip(train_proto, train_service, train_state)):\n",
    "#     groups[(p,s,st)].append(i)\n",
    "\n",
    "# edge_source = []\n",
    "# edge_target = []\n",
    "# # For each group, connect all nodes in that group fully\n",
    "# # But if the group is huge, try to maybe limit the number of edges\n",
    "# for triple, indices in groups.items():\n",
    "#     if len(indices) > 1:\n",
    "#         # connect all pairs in 'indices'\n",
    "#         # for large groups, you might want to do a random subset \n",
    "#         # or skip a big group to avoid excessive memory usage\n",
    "#         for i in range(len(indices) - 1):\n",
    "#             for j in range(i+1, len(indices)):\n",
    "#                 node_i = indices[i]\n",
    "#                 node_j = indices[j]\n",
    "#                 edge_source.append(node_i)\n",
    "#                 edge_target.append(node_j)\n",
    "#                 edge_source.append(node_j)\n",
    "#                 edge_target.append(node_i)\n",
    "\n",
    "# edge_index_train = np.vstack((edge_source, edge_target))\n",
    "# edge_index_train = torch.tensor(edge_index_train, dtype=torch.long)\n",
    "\n",
    "# print(\"edge_index_train shape:\", edge_index_train.shape)\n",
    "\n",
    "# Code above did not work well, maybe LOTS OF ADJACENCY\n",
    "\n",
    "# To reduce the combinatorial explosion we set thresholds.\n",
    "VERBOSE = True\n",
    "MAX_NODES_PER_GROUP = 500   # if a group has more than this, sample nodes\n",
    "MAX_EDGES_PER_GROUP = 10000 # if full connectivity would produce too many edges, sample pairs\n",
    "\n",
    "def create_edge_index(proto, service, state, max_nodes=MAX_NODES_PER_GROUP, \n",
    "                      max_edges=MAX_EDGES_PER_GROUP, verbose=False):\n",
    "    groups = defaultdict(list)\n",
    "    for i, (p, s, st) in enumerate(zip(proto, service, state)):\n",
    "        groups[(p, s, st)].append(i)\n",
    "    \n",
    "    if verbose:\n",
    "        num_groups = len(groups)\n",
    "        max_group_size = max(len(idxs) for idxs in groups.values())\n",
    "        print(f\"Total groups: {num_groups}, Maximum group size: {max_group_size}\")\n",
    "    \n",
    "    edge_source = []\n",
    "    edge_target = []\n",
    "    \n",
    "    for triple, indices in groups.items():\n",
    "        group_size = len(indices)\n",
    "        if group_size <= 1:\n",
    "            continue  # no edge possible\n",
    "        \n",
    "        # If the group is huge (YES!!!), sample a subset of nodes first\n",
    "        if group_size > max_nodes:\n",
    "            if verbose:\n",
    "                print(f\"Group {triple} has {group_size} nodes; sampling {max_nodes} nodes.\")\n",
    "            indices = random.sample(indices, max_nodes)\n",
    "            group_size = max_nodes\n",
    "\n",
    "        # Compute total possible undirected edges (each added in both directions)\n",
    "        total_edges = group_size * (group_size - 1) // 2\n",
    "        if total_edges > max_edges:\n",
    "            # sample a fixed number of edges\n",
    "            if verbose:\n",
    "                print(f\"Group {triple} (size {group_size}) would produce {total_edges} edges; sampling {max_edges} pairs.\")\n",
    "            all_pairs = list(itertools.combinations(indices, 2))\n",
    "            sampled_pairs = random.sample(all_pairs, max_edges)\n",
    "            for i, j in sampled_pairs:\n",
    "                edge_source.extend([i, j])\n",
    "                edge_target.extend([j, i])\n",
    "        else:\n",
    "            # Full connectivity for the group\n",
    "            for i in range(group_size - 1):\n",
    "                for j in range(i + 1, group_size):\n",
    "                    edge_source.extend([indices[i], indices[j]])\n",
    "                    edge_target.extend([indices[j], indices[i]])\n",
    "    # Stack into array: shape (2, num_edges)\n",
    "    return np.vstack((edge_source, edge_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c2128c-236e-4f0b-8ec1-b10c2f053cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups: 169, Maximum group size: 44185\n",
      "Group (113, 0, 4) has 44185 nodes; sampling 500 nodes.\n",
      "Group (113, 0, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 3, 4) has 3419 nodes; sampling 500 nodes.\n",
      "Group (113, 3, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 9, 4) has 5008 nodes; sampling 500 nodes.\n",
      "Group (113, 9, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 5, 4) has 18702 nodes; sampling 500 nodes.\n",
      "Group (113, 5, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 0, 5) has 12043 nodes; sampling 500 nodes.\n",
      "Group (119, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (6, 0, 5) has 2769 nodes; sampling 500 nodes.\n",
      "Group (6, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (78, 0, 5) has 1770 nodes; sampling 500 nodes.\n",
      "Group (78, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 4, 4) has 3993 nodes; sampling 500 nodes.\n",
      "Group (113, 4, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 2, 2) has 7592 nodes; sampling 500 nodes.\n",
      "Group (119, 2, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 0, 2) has 3723 nodes; sampling 500 nodes.\n",
      "Group (119, 0, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 11, 4) has 1292 nodes; sampling 500 nodes.\n",
      "Group (113, 11, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (78, 0, 7) has 825 nodes; sampling 500 nodes.\n",
      "Group (78, 0, 7) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 2, 5) has 39429 nodes; sampling 500 nodes.\n",
      "Group (119, 2, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 0, 2) has 1098 nodes; sampling 500 nodes.\n",
      "Group (113, 0, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 0, 7) has 856 nodes; sampling 500 nodes.\n",
      "Group (113, 0, 7) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (110, 0, 5) (size 201) would produce 20100 edges; sampling 10000 pairs.\n",
      "Group (120, 0, 5) has 12084 nodes; sampling 500 nodes.\n",
      "Group (120, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (111, 0, 5) (size 201) would produce 20100 edges; sampling 10000 pairs.\n",
      "Group (90, 0, 5) (size 200) would produce 19900 edges; sampling 10000 pairs.\n",
      "Group (100, 0, 5) (size 193) would produce 18528 edges; sampling 10000 pairs.\n",
      "Group (71, 0, 5) (size 201) would produce 20100 edges; sampling 10000 pairs.\n",
      "Group (97, 0, 5) has 620 nodes; sampling 500 nodes.\n",
      "Group (97, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (80, 0, 5) (size 201) would produce 20100 edges; sampling 10000 pairs.\n",
      "Group (3, 0, 5) (size 300) would produce 44850 edges; sampling 10000 pairs.\n",
      "Group (52, 0, 5) (size 201) would produce 20100 edges; sampling 10000 pairs.\n",
      "Group (32, 0, 5) (size 225) would produce 25200 edges; sampling 10000 pairs.\n",
      "Group (97, 0, 2) has 530 nodes; sampling 500 nodes.\n",
      "Group (97, 0, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 7, 4) has 1100 nodes; sampling 500 nodes.\n",
      "Group (113, 7, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 2, 7) (size 228) would produce 25878 edges; sampling 10000 pairs.\n",
      "edge_index_train shape: torch.Size([2, 1751590])\n"
     ]
    }
   ],
   "source": [
    "# For training data\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "train_proto   = train_df['proto'].values\n",
    "train_service = train_df['service'].values\n",
    "train_state   = train_df['state'].values\n",
    "\n",
    "edge_index_train_np = create_edge_index(train_proto, train_service, train_state, \n",
    "                                          max_nodes=MAX_NODES_PER_GROUP, \n",
    "                                          max_edges=MAX_EDGES_PER_GROUP, verbose=VERBOSE)\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "print(\"edge_index_train shape:\", edge_index_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b556fe1-40fc-4580-9e32-076f46f917fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[175341, 42], edge_index=[2, 1751590], y=[175341])\n",
      "Train data: #nodes = 175341 #edges = 1751590 num_features = 42\n"
     ]
    }
   ],
   "source": [
    "# Build the PyG Data for training\n",
    "x_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "train_data = Data(\n",
    "    x=x_train_tensor,\n",
    "    y=y_train_tensor,\n",
    "    edge_index=edge_index_train\n",
    ")\n",
    "print(train_data)\n",
    "print(\"Train data: #nodes =\", train_data.num_nodes, \n",
    "      \"#edges =\", train_data.num_edges, \n",
    "      \"num_features =\", train_data.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eff3346-4d65-4c07-b6f9-f0065de94ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups: 165, Maximum group size: 25629\n",
      "Group (119, 0, 5) has 6362 nodes; sampling 500 nodes.\n",
      "Group (119, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (6, 0, 5) has 926 nodes; sampling 500 nodes.\n",
      "Group (6, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 0, 4) has 25629 nodes; sampling 500 nodes.\n",
      "Group (113, 0, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 5, 4) has 8270 nodes; sampling 500 nodes.\n",
      "Group (113, 5, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 0, 7) has 1655 nodes; sampling 500 nodes.\n",
      "Group (113, 0, 7) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (78, 0, 5) has 524 nodes; sampling 500 nodes.\n",
      "Group (78, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (97, 0, 5) (size 205) would produce 20910 edges; sampling 10000 pairs.\n",
      "Group (113, 3, 4) has 1541 nodes; sampling 500 nodes.\n",
      "Group (113, 3, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 4, 4) has 1390 nodes; sampling 500 nodes.\n",
      "Group (113, 4, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 9, 4) has 1836 nodes; sampling 500 nodes.\n",
      "Group (113, 9, 4) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 7, 4) (size 420) would produce 87990 edges; sampling 10000 pairs.\n",
      "Group (119, 2, 5) has 18263 nodes; sampling 500 nodes.\n",
      "Group (119, 2, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 0, 2) has 1611 nodes; sampling 500 nodes.\n",
      "Group (119, 0, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (119, 2, 2) has 3083 nodes; sampling 500 nodes.\n",
      "Group (119, 2, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (120, 0, 5) has 3515 nodes; sampling 500 nodes.\n",
      "Group (120, 0, 5) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Group (113, 11, 4) (size 203) would produce 20503 edges; sampling 10000 pairs.\n",
      "Group (78, 0, 7) (size 152) would produce 11476 edges; sampling 10000 pairs.\n",
      "Group (113, 0, 2) has 2043 nodes; sampling 500 nodes.\n",
      "Group (113, 0, 2) (size 500) would produce 124750 edges; sampling 10000 pairs.\n",
      "Data(x=[82332, 42], edge_index=[2, 536916], y=[82332])\n",
      "Test data: #nodes = 82332 #edges = 536916 num_features = 42\n"
     ]
    }
   ],
   "source": [
    "# --- For test data ---\n",
    "test_proto   = test_df['proto'].values\n",
    "test_service = test_df['service'].values\n",
    "test_state   = test_df['state'].values\n",
    "\n",
    "edge_index_test_np = create_edge_index(test_proto, test_service, test_state, \n",
    "                                         max_nodes=MAX_NODES_PER_GROUP, \n",
    "                                         max_edges=MAX_EDGES_PER_GROUP, verbose=VERBOSE)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_data = Data(\n",
    "    x=x_test_tensor,\n",
    "    y=y_test_tensor,\n",
    "    edge_index=edge_index_test\n",
    ")\n",
    "print(test_data)\n",
    "print(\"Test data: #nodes =\", test_data.num_nodes, \n",
    "      \"#edges =\", test_data.num_edges, \n",
    "      \"num_features =\", test_data.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6363d4e-4e39-48b6-bb30-390943e11a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06e73e2-adfd-4514-8040-9bba27bfad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_features = train_data.num_features\n",
    "num_classes = len(np.unique(y_train))\n",
    "hidden_channels = 64\n",
    "\n",
    "model = GCN(num_features, hidden_channels, num_classes).to(device)\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44783374-befa-47bb-9984-09bdac45f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss = 0.8727, Time = 0.30 sec\n",
      "Epoch 02, Loss = 0.8530, Time = 0.06 sec\n",
      "Epoch 03, Loss = 0.8347, Time = 0.06 sec\n",
      "Epoch 04, Loss = 0.8186, Time = 0.06 sec\n",
      "Epoch 05, Loss = 0.8042, Time = 0.05 sec\n",
      "Epoch 06, Loss = 0.7913, Time = 0.05 sec\n",
      "Epoch 07, Loss = 0.7809, Time = 0.05 sec\n",
      "Epoch 08, Loss = 0.7726, Time = 0.05 sec\n",
      "Epoch 09, Loss = 0.7660, Time = 0.05 sec\n",
      "Epoch 10, Loss = 0.7595, Time = 0.05 sec\n",
      "Epoch 11, Loss = 0.7522, Time = 0.06 sec\n",
      "Epoch 12, Loss = 0.7443, Time = 0.05 sec\n",
      "Epoch 13, Loss = 0.7371, Time = 0.05 sec\n",
      "Epoch 14, Loss = 0.7313, Time = 0.05 sec\n",
      "Epoch 15, Loss = 0.7265, Time = 0.05 sec\n",
      "Epoch 16, Loss = 0.7219, Time = 0.05 sec\n",
      "Epoch 17, Loss = 0.7177, Time = 0.06 sec\n",
      "Epoch 18, Loss = 0.7141, Time = 0.05 sec\n",
      "Epoch 19, Loss = 0.7108, Time = 0.05 sec\n",
      "Epoch 20, Loss = 0.7075, Time = 0.06 sec\n",
      "Epoch 21, Loss = 0.7042, Time = 0.05 sec\n",
      "Epoch 22, Loss = 0.7005, Time = 0.05 sec\n",
      "Epoch 23, Loss = 0.6966, Time = 0.05 sec\n",
      "Epoch 24, Loss = 0.6930, Time = 0.05 sec\n",
      "Epoch 25, Loss = 0.6898, Time = 0.05 sec\n",
      "Epoch 26, Loss = 0.6865, Time = 0.05 sec\n",
      "Epoch 27, Loss = 0.6835, Time = 0.05 sec\n",
      "Epoch 28, Loss = 0.6806, Time = 0.05 sec\n",
      "Epoch 29, Loss = 0.6776, Time = 0.06 sec\n",
      "Epoch 30, Loss = 0.6749, Time = 0.05 sec\n",
      "Epoch 31, Loss = 0.6722, Time = 0.05 sec\n",
      "Epoch 32, Loss = 0.6694, Time = 0.05 sec\n",
      "Epoch 33, Loss = 0.6667, Time = 0.05 sec\n",
      "Epoch 34, Loss = 0.6640, Time = 0.06 sec\n",
      "Epoch 35, Loss = 0.6615, Time = 0.05 sec\n",
      "Epoch 36, Loss = 0.6589, Time = 0.06 sec\n",
      "Epoch 37, Loss = 0.6563, Time = 0.05 sec\n",
      "Epoch 38, Loss = 0.6537, Time = 0.05 sec\n",
      "Epoch 39, Loss = 0.6512, Time = 0.05 sec\n",
      "Epoch 40, Loss = 0.6488, Time = 0.05 sec\n",
      "Epoch 41, Loss = 0.6466, Time = 0.05 sec\n",
      "Epoch 42, Loss = 0.6444, Time = 0.05 sec\n",
      "Epoch 43, Loss = 0.6423, Time = 0.05 sec\n",
      "Epoch 44, Loss = 0.6402, Time = 0.05 sec\n",
      "Epoch 45, Loss = 0.6382, Time = 0.06 sec\n",
      "Epoch 46, Loss = 0.6363, Time = 0.05 sec\n",
      "Epoch 47, Loss = 0.6344, Time = 0.05 sec\n",
      "Epoch 48, Loss = 0.6326, Time = 0.05 sec\n",
      "Epoch 49, Loss = 0.6307, Time = 0.05 sec\n",
      "Epoch 50, Loss = 0.6289, Time = 0.05 sec\n",
      "Epoch 51, Loss = 0.6271, Time = 0.05 sec\n",
      "Epoch 52, Loss = 0.6254, Time = 0.05 sec\n",
      "Epoch 53, Loss = 0.6238, Time = 0.05 sec\n",
      "Epoch 54, Loss = 0.6221, Time = 0.05 sec\n",
      "Epoch 55, Loss = 0.6205, Time = 0.05 sec\n",
      "Epoch 56, Loss = 0.6189, Time = 0.05 sec\n",
      "Epoch 57, Loss = 0.6174, Time = 0.05 sec\n",
      "Epoch 58, Loss = 0.6160, Time = 0.05 sec\n",
      "Epoch 59, Loss = 0.6145, Time = 0.05 sec\n",
      "Epoch 60, Loss = 0.6131, Time = 0.05 sec\n",
      "Epoch 61, Loss = 0.6117, Time = 0.05 sec\n",
      "Epoch 62, Loss = 0.6104, Time = 0.05 sec\n",
      "Epoch 63, Loss = 0.6091, Time = 0.05 sec\n",
      "Epoch 64, Loss = 0.6078, Time = 0.05 sec\n",
      "Epoch 65, Loss = 0.6066, Time = 0.06 sec\n",
      "Epoch 66, Loss = 0.6053, Time = 0.05 sec\n",
      "Epoch 67, Loss = 0.6041, Time = 0.05 sec\n",
      "Epoch 68, Loss = 0.6029, Time = 0.05 sec\n",
      "Epoch 69, Loss = 0.6018, Time = 0.05 sec\n",
      "Epoch 70, Loss = 0.6006, Time = 0.05 sec\n",
      "Epoch 71, Loss = 0.5995, Time = 0.05 sec\n",
      "Epoch 72, Loss = 0.5984, Time = 0.06 sec\n",
      "Epoch 73, Loss = 0.5974, Time = 0.05 sec\n",
      "Epoch 74, Loss = 0.5963, Time = 0.05 sec\n",
      "Epoch 75, Loss = 0.5953, Time = 0.05 sec\n",
      "Epoch 76, Loss = 0.5944, Time = 0.05 sec\n",
      "Epoch 77, Loss = 0.5937, Time = 0.05 sec\n",
      "Epoch 78, Loss = 0.5937, Time = 0.05 sec\n",
      "Epoch 79, Loss = 0.5947, Time = 0.05 sec\n",
      "Epoch 80, Loss = 0.5946, Time = 0.05 sec\n",
      "Epoch 81, Loss = 0.5906, Time = 0.05 sec\n",
      "Epoch 82, Loss = 0.5894, Time = 0.05 sec\n",
      "Epoch 83, Loss = 0.5909, Time = 0.05 sec\n",
      "Epoch 84, Loss = 0.5884, Time = 0.05 sec\n",
      "Epoch 85, Loss = 0.5868, Time = 0.05 sec\n",
      "Epoch 86, Loss = 0.5877, Time = 0.05 sec\n",
      "Epoch 87, Loss = 0.5859, Time = 0.05 sec\n",
      "Epoch 88, Loss = 0.5845, Time = 0.05 sec\n",
      "Epoch 89, Loss = 0.5849, Time = 0.05 sec\n",
      "Epoch 90, Loss = 0.5836, Time = 0.05 sec\n",
      "Epoch 91, Loss = 0.5824, Time = 0.05 sec\n",
      "Epoch 92, Loss = 0.5824, Time = 0.05 sec\n",
      "Epoch 93, Loss = 0.5816, Time = 0.05 sec\n",
      "Epoch 94, Loss = 0.5804, Time = 0.05 sec\n",
      "Epoch 95, Loss = 0.5800, Time = 0.05 sec\n",
      "Epoch 96, Loss = 0.5796, Time = 0.05 sec\n",
      "Epoch 97, Loss = 0.5785, Time = 0.05 sec\n",
      "Epoch 98, Loss = 0.5778, Time = 0.05 sec\n",
      "Epoch 99, Loss = 0.5776, Time = 0.05 sec\n",
      "Epoch 100, Loss = 0.5769, Time = 0.05 sec\n"
     ]
    }
   ],
   "source": [
    "# Actually train \n",
    "import time\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data.x, train_data.edge_index)\n",
    "    loss = F.cross_entropy(out, train_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    loss_val = train_one_epoch()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch:02d}, Loss = {loss_val:.4f}, Time = {elapsed:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a83d6031-34aa-43b2-90c0-e7ee77772f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix (Train) ===\n",
      "[[  223     0     0  1559     1     0   172    45     0     0]\n",
      " [    0     0     0  1553    47     0    13   133     0     0]\n",
      " [    3     0    13 11352   288     3    72   527     6     0]\n",
      " [   40     0     5 30627  1349    15   377   978     2     0]\n",
      " [   14     0     4  2236 12849    18  2024  1036     3     0]\n",
      " [    4     0     0   692   140 39090    14    59     1     0]\n",
      " [  124     0     0  1278  6325     2 47858   413     0     0]\n",
      " [    4     0     0  3085   638     1   185  6578     0     0]\n",
      " [    0     0     0   113   318     0    32   660    10     0]\n",
      " [    2     0     0   109    17     0     0     2     0     0]]\n",
      "\n",
      "=== Classification Report (Train) ===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.54      0.11      0.18      2000\n",
      "      Backdoor       0.00      0.00      0.00      1746\n",
      "           DoS       0.59      0.00      0.00     12264\n",
      "      Exploits       0.58      0.92      0.71     33393\n",
      "       Fuzzers       0.58      0.71      0.64     18184\n",
      "       Generic       1.00      0.98      0.99     40000\n",
      "        Normal       0.94      0.85      0.90     56000\n",
      "Reconnaissance       0.63      0.63      0.63     10491\n",
      "     Shellcode       0.45      0.01      0.02      1133\n",
      "         Worms       0.00      0.00      0.00       130\n",
      "\n",
      "      accuracy                           0.78    175341\n",
      "     macro avg       0.53      0.42      0.41    175341\n",
      "  weighted avg       0.79      0.78      0.75    175341\n",
      "\n",
      "\n",
      "=== Confusion Matrix (Test) ===\n",
      "[[    0     0    13   522    92    13    37     0     0     0]\n",
      " [    0     0    12   426    98    13     8    26     0     0]\n",
      " [    2     0    51  3545   283    28    48   131     1     0]\n",
      " [   23     0    47  9702   764    43   325   228     0     0]\n",
      " [    0     0    22  1246  3631    24   947   192     0     0]\n",
      " [    0     0     2   468   512 17802    23    64     0     0]\n",
      " [  376     0     2  2077  9127     0 24328  1089     1     0]\n",
      " [    0     0     4   756   346     2    91  2297     0     0]\n",
      " [    0     0     0    37   118     0    12   210     1     0]\n",
      " [    0     0     0    35     8     0     0     1     0     0]]\n",
      "\n",
      "=== Classification Report (Test) ===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.00      0.00      0.00       677\n",
      "      Backdoor       0.00      0.00      0.00       583\n",
      "           DoS       0.33      0.01      0.02      4089\n",
      "      Exploits       0.52      0.87      0.65     11132\n",
      "       Fuzzers       0.24      0.60      0.35      6062\n",
      "       Generic       0.99      0.94      0.97     18871\n",
      "        Normal       0.94      0.66      0.77     37000\n",
      "Reconnaissance       0.54      0.66      0.59      3496\n",
      "     Shellcode       0.33      0.00      0.01       378\n",
      "         Worms       0.00      0.00      0.00        44\n",
      "\n",
      "      accuracy                           0.70     82332\n",
      "     macro avg       0.39      0.37      0.34     82332\n",
      "  weighted avg       0.78      0.70      0.71     82332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91965\\anaconda3\\envs\\infra-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91965\\anaconda3\\envs\\infra-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91965\\anaconda3\\envs\\infra-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91965\\anaconda3\\envs\\infra-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91965\\anaconda3\\envs\\infra-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\91965\\anaconda3\\envs\\infra-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = data.y.cpu().numpy()\n",
    "    return pred, labels\n",
    "\n",
    "# Evaluate on TRAIN data\n",
    "train_pred, train_labels = evaluate(train_data)\n",
    "cm_train = confusion_matrix(train_labels, train_pred)\n",
    "print(\"\\n=== Confusion Matrix (Train) ===\")\n",
    "print(cm_train)\n",
    "print(\"\\n=== Classification Report (Train) ===\")\n",
    "print(classification_report(train_labels, train_pred, target_names=attack_cat_encoder.classes_))\n",
    "\n",
    "# Evaluate on TEST data\n",
    "test_pred, test_labels = evaluate(test_data)\n",
    "cm_test = confusion_matrix(test_labels, test_pred)\n",
    "print(\"\\n=== Confusion Matrix (Test) ===\")\n",
    "print(cm_test)\n",
    "print(\"\\n=== Classification Report (Test) ===\")\n",
    "print(classification_report(test_labels, test_pred, target_names=attack_cat_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835c2bd7-909a-49bb-8914-ffecab87d6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and weights saved locally!\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model, \"gnn_model_actual_beta.pt\")\n",
    "\n",
    "# Save weights\n",
    "torch.save(model.state_dict(), \"gnn_model_actual_beta_weights.pt\")\n",
    "\n",
    "print(\"\\nModel and weights saved locally!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
